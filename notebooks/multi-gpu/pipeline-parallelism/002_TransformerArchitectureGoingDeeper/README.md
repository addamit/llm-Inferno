# What are we partitioning 

# Layer-by-layer breakdown of decoder transformers
# Memory footprint of each component (embeddings, attention, MLP, layer norm)
# Activation patterns and intermediate tensors
# Where the computational bottlenecks are
# How different layers behave differently (early vs late layers)

# Code Section:
- Memory tracking
- Layer profiling tools
- Visualization of memory usage per layer

